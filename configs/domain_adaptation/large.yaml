model_name: bert-large-uncased
groupping:
    do: false
    chunk_size: 512

mlm_probability: 0.15
training:
    num_epochs: 5
    train_batch_size: 32
    eval_batch_size: 16
    gradient_accumulation_steps: 4
    lr: 0.00001
    warmup_ratio: 0.03
    weight_decay: 0.01
    max_grad_norm: 1.
    early_stopping_patience: 3