domain_adaptation:
  model_name: bert-base-uncased
  groupping:
    do: false
    chunk_size: 512
  mlm_probability: 0.15
  training:
    num_epochs: 3
    train_batch_size: 128
    eval_batch_size: 64
    gradient_accumulation_steps: 1
    lr: 1.0e-05
    warmup_ratio: 0.03
    weight_decay: 0.01
    max_grad_norm: 1.0
    early_stopping_patience: 3
output_dir: outputs
offline_mode: false
cache_dir: cache
seed: 123456
experiment:
  al: true
  num_labeled: 1000
  lambda_: 0.33
multiple_experiments:
  do_domain_adaptation: false
  al:
  - true
  - false
  lambda_:
  - 0.17
  - 0.33
  - 0.5
  - 0.67
  - 0.83
  num_labeled:
  - 10
  - 25
  - 50
  - 100
  - 250
  - 500
  - 1000
  - 1500
  - 2500
  seed:
  - 42
  - 23419
  - 123456
model:
  name: meta-llama/Meta-Llama-3-8B
  use_tllm: false
data:
  dataset: xsum
  input_column_name: document
  output_column_name: summary
  model_max_length: 1024
  max_tokens: 42
  train_size: -1
  test_size: 1000
method:
  lambda_: 0.83
  num_neighbors: 3
  calculate_unlabeled_scores: false
  num_to_select: 2500
training:
  num_epochs: 50
  train_batch_size: 16
  eval_batch_size: 8
  gradient_accumulation_steps: 4
  lr: 3.0e-05
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_grad_norm: 1.0
  early_stopping_patience: 5
inference:
  batch_size: 8
embeddings:
  batch_size: 16
