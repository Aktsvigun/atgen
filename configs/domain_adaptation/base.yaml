model_name: bert-base-uncased
groupping:
    do: false
    chunk_size: 512

mlm_probability: 0.15
training:
    num_epochs: 3
    train_batch_size: 128
    eval_batch_size: 64
    gradient_accumulation_steps: 1
    lr: 0.00001
    warmup_ratio: 0.03
    weight_decay: 0.01
    max_grad_norm: 1.
    early_stopping_patience: 3