labeller:
  type: golden
data:
  dataset: SpeedOfMagic/gigaword_tiny
  fetch_kwargs: {}
  has_test: false
  input_column_name: document
  input_max_length: 100
  output_column_name: output
  output_max_length: 20
  prompt: ''
  test_subset_size: null
  train_subset_size: null
name: test
output_dir: outputs
offline_mode: false
cache_dir: cache
seed: 42
al:
  strategy: random
  init_query_size: 10
  query_size: 10
  num_iterations: 1
  subsample_size: -1
  required_performance:
    rouge1: 0.5
  budget: 10
  strategy_kwargs:
    lambda_: 0.17
model:
  checkpoint: stabilityai/stablelm-2-zephyr-1_6b
  quantize: false
  model_max_length: ${data.input_max_length}
  dtype: float16
  separator: '<|assistant|>

    '
  peft:
    use: true
    r: 64
    lora_alpha: 64
    lora_dropout: 0.1
    bias: none
    modules: null
training:
  dev_split_size: 0.2
  hyperparameters:
    num_epochs: 1
    train_batch_size: 64
    eval_batch_size: 16
    gradient_accumulation_steps: 1
    lr: 3.0e-05
    warmup_ratio: 0.03
    weight_decay: 0.01
    max_grad_norm: 1.0
    early_stopping_patience: 5
    optimizer: adamw_hf
inference:
  framework: transformers
  max_new_tokens: ${data.output_max_length}
  batch_size: 8
  gpu_memory_utilization: 0.5
  temperature: 0.0
  top_p: 0.5
